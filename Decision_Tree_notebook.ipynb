{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7_-Yh9uO8Ujd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LYDs3tHJDpdz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dtree :\n",
    "  def __init__(self , \n",
    "               X = None ,\n",
    "               y = None ,\n",
    "               depth = 0,\n",
    "               MAX_DEPTH = 3,\n",
    "               error = \"entropy\",\n",
    "               features = None ,\n",
    "               label = None,\n",
    "               feature = None, \n",
    "               Type = None,\n",
    "               AllFeatures = [],\n",
    "               value =None,\n",
    "               NextSplitBy = None,\n",
    "               confidenceLevel = 0.05,\n",
    "               minSampleSize = 20\n",
    "               ):\n",
    "    \n",
    "        self.children = {}\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.MAX_DEPTH = MAX_DEPTH\n",
    "        self.AllFeatures = AllFeatures\n",
    "        self.NextSplitBy = NextSplitBy\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.label = label\n",
    "        self.Type = Type\n",
    "        \n",
    "        self.confidenceLevel = confidenceLevel\n",
    "        self.depth = depth\n",
    "        self.error = error\n",
    "        self.minSampleSize = minSampleSize\n",
    "\n",
    "        self.Bulid()\n",
    "\n",
    "\n",
    "  def Bulid (self):\n",
    "    \n",
    "    if self.depth >= self.MAX_DEPTH:\n",
    "        self.label = max (dict (self.y.value_counts()))\n",
    "        self.Type = \"leaf\"\n",
    "        return \n",
    "\n",
    "        \n",
    "\n",
    "    if len (self.y.unique()) == 1: #one class left in Y\n",
    "      self.label = max (dict (self.y.value_counts()))\n",
    "      self.Type = \"leaf\"\n",
    "      return \n",
    "\n",
    "\n",
    "    if len(self.AllFeatures) == 0: # No features left\n",
    "      self.label = max (dict (self.y.value_counts()))\n",
    "      self.Type = \"leaf\"\n",
    "      return\n",
    "\n",
    "\n",
    "    if len (self.y) < self.minSampleSize :   #min sample examples\n",
    "      self.Type = \"leaf\"\n",
    "      self.label = max (dict (self.y.value_counts()))\n",
    "      return \n",
    "\n",
    "\n",
    "\n",
    "    self.NextSplitBy = self.MakeSplitDecision(self.AllFeatures , self.error)  \n",
    "\n",
    "    if self.NextSplitBy is None:\n",
    "      self.AllFeatures = []\n",
    "      self.label = self.y.unique()[0]\n",
    "      self.Type =\"leaf\"\n",
    "      return \n",
    "\n",
    "    if not (self.chi_square(self.NextSplitBy , self.X , self.y)):\n",
    "        self.AllFeatures =[]\n",
    "        return\n",
    "\n",
    "    if len(self.y.unique()) > 1 : \n",
    "        \n",
    "        possibleValues = self.X[self.NextSplitBy].unique()\n",
    "\n",
    "        for v in possibleValues:\n",
    "          \n",
    "          SubsetX = self.X[self.X[self.NextSplitBy] == v] # return rows where feature == specific values\n",
    "          SubsetY = self.y[SubsetX.index]\n",
    "     \n",
    "          m = [f for f in self.AllFeatures if f != self.NextSplitBy]\n",
    "\n",
    "          child = Dtree(X = SubsetX.copy() ,\n",
    "                        y = SubsetY.copy() ,\n",
    "                        error = self.error,\n",
    "                        depth = self.depth +1 ,\n",
    "                        label = max (dict (SubsetY.value_counts())),\n",
    "                        AllFeatures = m,\n",
    "                        MAX_DEPTH = self.MAX_DEPTH,\n",
    "                        feature = self.NextSplitBy,\n",
    "                        value = v,\n",
    "                        Type = \"internal\",\n",
    "                        confidenceLevel = self.confidenceLevel,\n",
    "                        minSampleSize = self.minSampleSize\n",
    "                        )\n",
    "   \n",
    "          self.children[v]= child # append to children\n",
    "        \n",
    "        for v in possibleValues:          \n",
    "          self.children[v].Bulid()\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "    return self\n",
    "\n",
    "  \n",
    "  def predict (self , df):\n",
    "    \"\"\" recieves a dataframe and returns a list of predictions \"\"\"\n",
    "\n",
    "\n",
    "    predictions = []\n",
    "    setOfFeature= set()\n",
    "    for indx in range(len(df)):\n",
    "\n",
    "        pred = self.predictOne(dict(df.iloc[indx]).copy()) \n",
    "\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "  def predictOne(self , row) -> str:\n",
    "\n",
    "\n",
    "      node = self\n",
    "      label = \"Undefined\"\n",
    "        \n",
    "      features = None #self.AllFeatures\n",
    "\n",
    "      while (node is not None ):\n",
    "\n",
    "        if len(node.children) != 0 :\n",
    "\n",
    "              branchName = row[node.NextSplitBy]\n",
    "              child =None\n",
    "              try :\n",
    "                child = node.children[branchName] \n",
    "              except:\n",
    "                return node.label\n",
    "\n",
    "              row.pop (node.NextSplitBy)\n",
    "\n",
    "              node = child \n",
    " \n",
    "        else:\n",
    "              label = node.label\n",
    "              break\n",
    "\n",
    "      return label \n",
    "\n",
    "\n",
    "\n",
    "  def MakeSplitDecision (self , features , error):\n",
    "    \"\"\"\n",
    "    Perform spliting on the tree until:\n",
    "        1- the maximum depth is reached, \n",
    "        2- one class remains, or\n",
    "        3- A performance metric is achieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    #determine the best split based on the features\n",
    "\n",
    "    BestFeature = None\n",
    "    BestInformationGain = -1\n",
    "    BestFeatureList = []\n",
    "\n",
    "    #consider all error metrics for the split. For a list of features, \n",
    "    #we take the feature that satisfies most of error metrics\n",
    "\n",
    "    if error == \"all\" or error == \"All\": \n",
    "      for err in {\"Entropy\" , \"MCE\" , \"gini\"}:\n",
    "        BestFeatureList = {}\n",
    "\n",
    "        for f in features:  \n",
    "\n",
    "          IG = self.CalculateIG(f , err)\n",
    "\n",
    "          if BestInformationGain < IG:\n",
    "            BestFeature = f\n",
    "            BestInformationGain = IG\n",
    "\n",
    "        BestFeatureList[err] = BestFeature\n",
    "\n",
    "      return max(Counter(BestFeatureList.values())) #return the feature suggested by most error metrics\n",
    "\n",
    "    else:\n",
    "      for f in features:  \n",
    "\n",
    "          IG = self.CalculateIG(f , error)\n",
    "\n",
    "          if BestInformationGain < IG:\n",
    "            BestFeature = f\n",
    "            BestInformationGain = IG\n",
    "\n",
    "      return BestFeature\n",
    "\n",
    "\n",
    "  def chi_square(self, feature , X , y  ):\n",
    "        \n",
    "    \"\"\"\n",
    "    Given a feature, return True if that feature useful for performing the next split.\n",
    "    return False, if not.\n",
    "    \"\"\"\n",
    "    if self.confidenceLevel == 0: #always expand\n",
    "        return True\n",
    "    \n",
    "    possibleValues = set(X[feature])\n",
    "\n",
    "    X_counts = dict (X[feature].value_counts()) \n",
    "\n",
    "    y_counts = dict (Counter(y))\n",
    "\n",
    "\n",
    "    Sum = 0\n",
    "    for v in possibleValues:\n",
    "      SubsetX = X[X[feature] == v] \n",
    "      SubsetY = y[SubsetX.index] \n",
    "\n",
    "      SubsetY_counts = dict (Counter(SubsetY))\n",
    "\n",
    "\n",
    "      for key in y_counts.keys():\n",
    "        actual = SubsetY_counts.get(key , 0)\n",
    "        expected = len(SubsetY) * ((y_counts.get(key , 0) / len(y)))\n",
    "\n",
    "        Sum += ((actual - expected)**2)/expected\n",
    "\n",
    "\n",
    "    dFreedom = (len(y_counts) - 1) * (len(possibleValues) - 1)\n",
    "    \n",
    "    \n",
    "\n",
    "    chi_value = scipy.stats.chi2.ppf(1- self.confidenceLevel, df=dFreedom)\n",
    "\n",
    "\n",
    "    if (Sum > chi_value):\n",
    "      return True\n",
    "\n",
    "    return False\n",
    "  \n",
    "  def CalculateIG(self , feature , error ):\n",
    "    \"\"\"\n",
    "      loop over attribute's values to calculate the maximum Information Gain.\n",
    "    \"\"\"\n",
    "\n",
    "    #Get unique values for that attributes:\n",
    "    Xtemp = self.X.copy()\n",
    "    ytemp = self.y.copy()\n",
    "\n",
    "    PossibleValuesForFeature= Xtemp[feature].unique()\n",
    "\n",
    "    IG = None\n",
    "\n",
    "    if error == \"MCE\" or error == \"mce\":\n",
    "      IG = self.MCE(Xtemp, ytemp)\n",
    "    elif error == \"entropy\" or error == \"Entropy\":\n",
    "      IG = self.Entropy(Xtemp, ytemp)\n",
    "    elif error == \"gini\" or error == \"Gini\":\n",
    "      IG = self.GiniIndex(Xtemp, ytemp)\n",
    "    else:\n",
    "      print(\"enter a valid error metric name , {MCE, entropy, Gini, or  All}.\")      \n",
    "    \n",
    "    \n",
    "\n",
    "    for value in PossibleValuesForFeature:\n",
    "      SubsetX = Xtemp[Xtemp[feature] == value] # return rows where feature == specific values\n",
    "      SubsetY = ytemp [SubsetX.index]\n",
    "\n",
    "      IG -=  (SubsetX.shape[0] / Xtemp.shape[0]) * self.Entropy(SubsetX, SubsetY) \n",
    "      if error == \"MCE\" or error == \"mce\":\n",
    "         IG -=  (SubsetX.shape[0] / Xtemp.shape[0])  * self.MCE(SubsetX, SubsetY) \n",
    "  \n",
    "         \n",
    "      elif error == \"entropy\" or error == \"Entropy\":\n",
    "        IG -= (SubsetX.shape[0] / Xtemp.shape[0]) * self.Entropy(SubsetX, SubsetY) \n",
    "         \n",
    "\n",
    "      elif error == \"gini\" or error == \"Gini\":\n",
    "        IG -=  (SubsetX.shape[0] / Xtemp.shape[0]) * self.GiniIndex(SubsetX, SubsetY) \n",
    "         \n",
    "\n",
    "      else:\n",
    "        print(\"enter a valid error metric name , {MCE, entropy, Gini, or  All}.\") \n",
    "\n",
    "    return IG\n",
    "\n",
    "\n",
    "\n",
    "  def GiniIndex(self, Xtemp, ytemp ):\n",
    "    \n",
    "    gini = 0\n",
    "    \n",
    "    for c in ytemp.unique(): #returns the rows where class == c   \n",
    "      splitCandidate = Xtemp[ytemp[:] == c] \n",
    "\n",
    "      L1 = len(splitCandidate) \n",
    "      L2 = len(Xtemp)\n",
    "\n",
    "      ratio = L1 / L2\n",
    "      gini += (ratio ** 2)\n",
    "\n",
    "    return 1- gini \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def Entropy(self, XTemp , yTemp):\n",
    "    \"\"\"\n",
    "    For attribute x_i, calculate the value of entropy \n",
    "    \"\"\"\n",
    "\n",
    "    entropy = 0\n",
    "    for c in yTemp.unique(): #returns the rows where class == c   \n",
    "      splitCandidate= XTemp[yTemp[:] == c] \n",
    "\n",
    "      L1 = len(splitCandidate) \n",
    "      L2 = len(XTemp)\n",
    "\n",
    "      ratio = L1 / L2\n",
    "      entropy += ratio * np.log2(ratio)\n",
    "    \n",
    "\n",
    "    return -1 * entropy #take every class i and do (- p_i .log p_i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def MCE(self , Xtemp, ytemp):\n",
    "    \"\"\"\n",
    "     - Miss-Classification Error(MCE): measures the amount\n",
    "     of samples that classified with an incorrect label. \n",
    "\n",
    "     - calculating the MCE for each attributes (A) then \n",
    "     the attribute with minimum MCE is selected at each node.\n",
    "    \"\"\"\n",
    "    mceList = []\n",
    "    \n",
    "    for c in ytemp.unique(): #returns the rows where class == c   \n",
    "      splitCandidate = Xtemp[ytemp[:] == c] \n",
    "\n",
    "      L1 = len(splitCandidate) \n",
    "      L2 = len(Xtemp)\n",
    "\n",
    "      ratio = L1 / L2\n",
    "      mceList.append(ratio)\n",
    "\n",
    "    return 1- max (mceList) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2O6F78CeMIHy"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "\n",
    "  def __init__(self, error_metric= \"entropy\",\n",
    "               confidence_level = 0.05 ,\n",
    "               MAX_DEPTH = 3,\n",
    "               n_ensembles = 1,\n",
    "               stopping_critera = \"Chi_square\",\n",
    "               minimumSampleSize = 20) :\n",
    "    \n",
    "    self.forest = []\n",
    "    self.MAX_DEPTH = MAX_DEPTH\n",
    "    self.stopping_critera = stopping_critera\n",
    "    self.error_metric = error_metric\n",
    "    self.confidence_level = confidence_level\n",
    "    self.minimumSampleSize = minimumSampleSize\n",
    "    self.n_ensembles = n_ensembles\n",
    "      \n",
    "  def fit (self, X_temp, y_temp):\n",
    "    \n",
    "    if self.n_ensembles == 1 :\n",
    "        Tree = Dtree (X = X_temp ,\n",
    "                       y= y_temp,\n",
    "                       AllFeatures= X_temp.columns,\n",
    "                       Type = \"root\",\n",
    "                       MAX_DEPTH = self.MAX_DEPTH,\n",
    "                       error = self.error_metric ,\n",
    "                       confidenceLevel = self.confidence_level,\n",
    "                       minSampleSize = self.minimumSampleSize)\n",
    "        \n",
    "        self.forest.append(Tree)\n",
    "        \n",
    "    elif self.n_ensembles > 1:\n",
    "        self.ensemble(X_temp, y_temp)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Error n_ensembles cannot be negative!')\n",
    "        \n",
    "    \n",
    "  def predict (self, X):\n",
    "    \n",
    "    ids = X.index  \n",
    "    predictions = {i:{\"IE\":0 ,\"EI\":0, \"N\":0 } for i in ids}\n",
    "    \n",
    "    for i in ids:  #iterate over all rows\n",
    "        \n",
    "        #predict row_i using all models\n",
    "        for tree in self.forest : \n",
    "            \n",
    "            # predict one row using tree_i\n",
    "            label = tree.predictOne(dict(X.loc[i])) \n",
    "            \n",
    "            # increase the predicted label counter\n",
    "            predictions[i][label] +=1    \n",
    "\n",
    "    #extract labels with max predicted count    \n",
    "    forestPredictions = [max(predictions[i],\n",
    "                             key=predictions[i].get) for i in predictions.keys()]\n",
    "\n",
    "    return np.array(forestPredictions)\n",
    "\n",
    "    \n",
    "\n",
    "  def accuracy_score(self, X_test , y_true):\n",
    "        y_pred = self.predict(X_test)\n",
    "        return np.sum(np.equal(y_true, y_pred)) / len(y_true)\n",
    "    \n",
    "\n",
    "  def ensemble (self, X, y):\n",
    "    \n",
    "    \n",
    "    #sampled rows should be attached with the correct label.\n",
    "    # We need to sample both X and y together.\n",
    "    df = X.copy()\n",
    "    df[\"label\"] = y\n",
    "    \n",
    "    # reset index to start from zero, pd.sample method takes ordered dataframe\n",
    "    df = df.reset_index(drop=True) \n",
    "    \n",
    "    \n",
    "    for i in range (self.n_ensembles):\n",
    "\n",
    "        df_sampled = df.sample(frac=1 , axis= 0 , replace=True).reset_index(drop=True) \n",
    "        \n",
    "        y_sampled = df_sampled[\"label\"]\n",
    "        X_sampled = df_sampled.drop(labels='label', axis=1)\n",
    "        \n",
    "        \n",
    "        Tree = Dtree (X = X_sampled,\n",
    "                       y= y_sampled,\n",
    "                       AllFeatures= X_sampled.columns,\n",
    "                       Type = \"root\",\n",
    "                       MAX_DEPTH = self.MAX_DEPTH,\n",
    "                       error = self.error_metric ,\n",
    "                       confidenceLevel = self.confidence_level,\n",
    "                       minSampleSize = self.minimumSampleSize)\n",
    "                      \n",
    "        self.forest.append(Tree)\n",
    "        print(\"Classifier #\", i+1, \"completed training: \")\n",
    "        print(\"\\tAccuracy on Training data: \", self.accuracy_score(X_sampled, y_sampled))\n",
    "        print()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gf77HEygq2uu"
   },
   "outputs": [],
   "source": [
    "\n",
    "def replace (s , replace, listOfStr):\n",
    "    \"\"\" given a string return a new a string with a char replaced randomly from a given list.\"\"\"\n",
    "    countReplace = s.count(replace)\n",
    "    for i in range (s.count(replace)):\n",
    "        s = s.replace(replace , random.choice(listOfStr) , 1 )\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def split(s: str ,chunk_size: int ):\n",
    "    \"\"\"Split a string into chunks based on chunk_size \"\"\"\n",
    "    \n",
    "    listStr =[]\n",
    "    \n",
    "    for i in range(0, len(s), chunk_size) :\n",
    "        listStr.append(s[i:i+chunk_size])\n",
    "\n",
    "    return listStr\n",
    "\n",
    "def n_grams(df , chunk_size=1): \n",
    "    \"\"\"Split the samples into features of grams based into chunk_size \"\"\"\n",
    "    \n",
    "    features = list(range(1, int( len(df[\"sample\"][0])/chunk_size ) +1 , 1))\n",
    "    List = [split(s , chunk_size) for s in df[\"sample\"]]\n",
    "    \n",
    "    df = pd.concat(\n",
    "        [df, pd.DataFrame(\n",
    "            List, \n",
    "            index=df.index, \n",
    "            columns=features) ] , axis=1 )\n",
    "\n",
    "    return df , features\n",
    "\n",
    "def preprocess (df, ngrams = 1 , yFlag = False , duplicates = \"no\", unknown=\"no\"):\n",
    "    \n",
    "    \"\"\"perform the preprocessing steps:\n",
    "        - replace ambigous letters\n",
    "        - generate features based on the n-gram model\n",
    "        - deal with duplicate rows\n",
    "        \n",
    "        parameters:\n",
    "            y_flag: True, if df contains labels. The labels will be extracted.\n",
    "            duplicates: if \"drop\", the duplicate rows will be dropped. Otherwise, they will be kept.\n",
    "            unknown: if \"drop\", rows with ambiguous letters will be dropped. \n",
    "                     Otherwise, they will be replaced based on \"replacements\" dictionary.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    replecements = {\"D\": [\"A\", \"G\", \"T\"],\n",
    "                \"N\": [\"A\", \"G\", \"C\", \"T\"],\n",
    "                \"S\": [\"C\" , \"G\"],\n",
    "                \"R\" : [\"A\" , \"G\"]}\n",
    "    \n",
    "    if duplicates == \"drop\":\n",
    "        df.drop_duplicates(subset=['sample'])\n",
    "        \n",
    "    if unknown == \"drop\":\n",
    "        unknowDNA = df[\"sample\"].apply(lambda x: (\"N\" in x) or (\"D\" in x) or (\"S\" in x) or (\"R\" in x))\n",
    "        df = df.drop( unknowDNA[unknowDNA ==True].index )\n",
    "    else:\n",
    "        df['sample'] = df['sample'].apply(lambda x: replace(x, 'D' , replecements[\"D\"]))\n",
    "        df['sample'] = df['sample'].apply(lambda x: replace(x, 'N' , replecements[\"N\"]))\n",
    "        df['sample'] = df['sample'].apply(lambda x: replace(x, 'S' , replecements[\"S\"]))\n",
    "        df['sample'] = df['sample'].apply(lambda x: replace(x, 'R' , replecements[\"R\"]))\n",
    "\n",
    "        \n",
    "    df, features = n_grams(df , chunk_size = ngrams)\n",
    "    X = df[features]\n",
    "    y =None\n",
    "    \n",
    "    if yFlag:\n",
    "        y = df[\"label\"]\n",
    "        \n",
    "    return X , y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nxS7ik8erKxq",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/Users/ajararweh/Downloads/train.csv\" , names = [\"id\", \"sample\", \"label\"])\n",
    "df = dataset.copy()\n",
    "\n",
    "X , y = preprocess(df, ngrams= 1 , yFlag=True, duplicates=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aJFZ6qPD9Sg",
    "outputId": "ba59e3af-cadc-41ca-fcab-b10ee05ba9a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier # 1 completed training: \n",
      "\tAccuracy on Training data:  0.998125\n",
      "\n",
      "Classifier # 2 completed training: \n",
      "\tAccuracy on Training data:  0.9825\n",
      "\n",
      "Train:  0.95625\n",
      "CV:  0.8875\n"
     ]
    }
   ],
   "source": [
    "X_train, X_CV, y_train, y_CV = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "\n",
    "\n",
    "classifier = DecisionTreeClassifier(error_metric=\"gini\" ,\n",
    "                                    minimumSampleSize =4 ,\n",
    "                                    MAX_DEPTH = 7,\n",
    "                                    confidence_level= 0.999,\n",
    "                                    n_ensembles=2)\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "print(\"Train: \",classifier.accuracy_score(X_train,y_train))\n",
    "print (\"CV: \"  , classifier.accuracy_score(X_CV, y_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"/Users/ajararweh/Downloads/test.csv\" , names = [\"id\", \"sample\"])\n",
    "X_test , _ = preprocess(testData , yFlag=False, duplicates=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame({\"id\":testData[\"id\"], \"class\":classifier.predict(X_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv(\"/Users/ajararweh/Downloads/submission31.csv\", index = False )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Decision Tree.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
